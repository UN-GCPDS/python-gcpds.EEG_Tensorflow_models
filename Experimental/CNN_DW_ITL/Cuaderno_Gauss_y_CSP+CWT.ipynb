{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cuaderno_Gauss_y_CSP+CWT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UN-GCPDS/python-gcpds.EEG_Tensorflow_models/blob/main/Experimental/CNN_DW_ITL/Cuaderno_Gauss_y_CSP%2BCWT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT2UZ_XIYU2l"
      },
      "source": [
        "## Cargar zip de los sujetos y funciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOaVqIHKYCIg",
        "outputId": "b4d3c40f-b4d0-4552-fba3-eac8c04f5cae"
      },
      "source": [
        "\n",
        "from pydrive.drive import GoogleDrive\n",
        "import io\n",
        "import zipfile\n",
        "drive = GoogleDrive()\n",
        "import os\n",
        "import numpy as np\n",
        "#from CNN_Diego import *\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "import matplotlib.pyplot as plt\n",
        "#--------------------------------------------------------------------------------------\n",
        "!pip install tf-keras-vis tensorflow\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from matplotlib import cm\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-keras-vis\n",
            "  Downloading tf_keras_vis-0.8.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 40 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 51 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (7.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (21.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (2.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (4.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (1.4.1)\n",
            "Collecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (5.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.40.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->tf-keras-vis) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tf-keras-vis) (2.4.7)\n",
            "Installing collected packages: deprecated, tf-keras-vis\n",
            "Successfully installed deprecated-1.2.13 tf-keras-vis-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCQrSp1EcojT"
      },
      "source": [
        "## Cargar CSP y CWT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwsOPvB2YaNN"
      },
      "source": [
        "#CSP + CWT GIGA 1 ventana, 4 bandas de frecuencia\n",
        "#url https://drive.google.com/file/d/1zvvtLD4x7TA340uKWv4YlNId8EbtQ14c/view?usp=sharing\n",
        "FILEID = \"1zvvtLD4x7TA340uKWv4YlNId8EbtQ14c\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O cspycwt.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip cspycwt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-9rZxgudDqj"
      },
      "source": [
        "## Cargar Gauss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqmRtoozY4A0"
      },
      "source": [
        "#Gauss 1 ventana, 4 bandas de frecuencia\n",
        "#url https://drive.google.com/file/d/1vqgbL06dKw-g7vNH0rQ2rvBL4n0BzkSw/view?usp=sharing\n",
        "FILEID = \"1vqgbL06dKw-g7vNH0rQ2rvBL4n0BzkSw\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O gauss.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip gauss.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g13nh_6rfABK"
      },
      "source": [
        "# Red CNN D&W"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBs17gOKZRAk"
      },
      "source": [
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.base import BaseEstimator\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        " \n",
        "\n",
        "def conv_to_list(arr):\n",
        "    x_pre = []\n",
        "    for i in range(arr.shape[-1]):\n",
        "        x_pre.append(arr[:, :, :, :, i])\n",
        "    return x_pre\n",
        "  \n",
        "def labels_convert(labels_pre, labels_true):\n",
        "    labels_conv = np.zeros(labels_pre.shape)\n",
        "    u = np.unique(labels_pre)\n",
        "    ve = []\n",
        "    for i in u:\n",
        "        val = stats.mode(labels_true[labels_pre == i], 0)[0]\n",
        "        ve.append(val[0])\n",
        "        labels_conv[labels_pre == i] = val\n",
        "    return labels_conv.astype('int')\n",
        " \n",
        "def plot_model_(history_):\n",
        "    pd.DataFrame(history_.history).plot(figsize=(5, 5))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        " \n",
        "class CNNrITL2(BaseEstimator):\n",
        "  def __init__(self, d=2, sigma=None, k=2, verbose=10, n_fil=1,\n",
        "                epochs=200, batch_size=128, lr=1e-3, sl='ritl',\n",
        "                lk=0.5, l1=1e-3, wi=True, sbj=1, plot_model=True):\n",
        "    self.verbose = verbose\n",
        "    self.sigma = sigma\n",
        "    self.d = d\n",
        "    self.n_fil = n_fil\n",
        "    self.k = k\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.lr = lr\n",
        "    self.sl = sl\n",
        "    self.lk = lk\n",
        "    self.l1 = l1\n",
        "    self.sbj = sbj\n",
        "    self.wi = wi\n",
        "    self.plot_model = plot_model\n",
        "      \n",
        "\n",
        "  def transform(self, X, *_):\n",
        "    return self.model.predict(X)\n",
        "\n",
        "  def fit_transform(self, X, y):\n",
        "    self.fit(X, y)\n",
        "    return self.transform(x)\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    self.model = self.main(X, y)\n",
        "    return self\n",
        "\n",
        "  def main(self, x, y):\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # deep and wide mode for BCI topoplots\n",
        "    if self.wi:\n",
        "      print(x.shape)\n",
        "      #batch0_ = [None] * x.shape[-1]\n",
        "      input_ = [None] * x.shape[-1]\n",
        "      conv0_ = [None] * x.shape[-1]\n",
        "      pool0_ = [None] * x.shape[-1]\n",
        "      batch1_ = [None] * x.shape[-1]\n",
        "      batch2_ = [None] * x.shape[-1]\n",
        "      conv1_ = [None] * x.shape[-1]\n",
        "      pool1_ = [None] * x.shape[-1]\n",
        "      batch3_ = [None] * x.shape[-1]\n",
        "      batch4_ = [None] * x.shape[-1]\n",
        "      conv2_ = [None] * x.shape[-1]\n",
        "      pool2_ = [None] * x.shape[-1]\n",
        "      batch5_ = [None] * x.shape[-1]\n",
        "      batch6_ = [None] * x.shape[-1]\n",
        "      for i in range(x.shape[-1]):\n",
        "        input_[i] = keras.layers.Input(shape=x.shape[1:-1])\n",
        "        #batch0_[i] = keras.layers.BatchNormalization()(input_[i])\n",
        "        # ------------------------------------------------------------------------------------------------------\n",
        "        conv0_[i] = keras.layers.Conv2D(filters=self.n_fil, kernel_size=5, strides=1, activation='relu',\n",
        "                                        padding='SAME', input_shape=x.shape[1:-1],\n",
        "                                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1, l2=self.l1),\n",
        "                                        kernel_initializer=\"GlorotNormal\",name='C1_bf_'+str(i))(input_[i])\n",
        "        batch1_[i] = keras.layers.BatchNormalization()(conv0_[i])\n",
        "        pool0_[i] = keras.layers.MaxPooling2D(pool_size=2)(batch1_[i])\n",
        "        batch2_[i] = keras.layers.BatchNormalization()(pool0_[i])\n",
        "        # ------------------------------------------------------------------------------------------------------\n",
        "        conv1_[i] = keras.layers.Conv2D(filters=int(self.n_fil*0.5), kernel_size=5, strides=1, activation='relu',\n",
        "                                        padding='SAME', input_shape=x.shape[1:-1],\n",
        "                                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1, l2=self.l1),\n",
        "                                        kernel_initializer=\"GlorotNormal\",name='C2_bf_'+str(i))(batch2_[i])\n",
        "      \n",
        "        batch3_[i] = keras.layers.BatchNormalization()(conv1_[i])\n",
        "        pool1_[i] = keras.layers.MaxPooling2D(pool_size=2)(batch3_[i])\n",
        "        batch4_[i] = keras.layers.BatchNormalization()(pool1_[i])\n",
        "        # ------------------------------------------------------------------------------------------------------\n",
        "        conv2_[i] = keras.layers.Conv2D(filters=int(self.n_fil*0.25), kernel_size=5, strides=1, activation='relu',\n",
        "                                        padding='SAME', input_shape=x.shape[1:-1],\n",
        "                                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1, l2=self.l1),\n",
        "                                        kernel_initializer=\"GlorotNormal\",name='C3_bf_'+str(i))(batch4_[i])\n",
        "      \n",
        "        batch5_[i] = keras.layers.BatchNormalization()(conv2_[i])\n",
        "        pool2_[i] = keras.layers.MaxPooling2D(pool_size=2)(batch5_[i])\n",
        "        batch6_[i] = keras.layers.BatchNormalization()(pool2_[i])\n",
        "        # ------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      concat = keras.layers.concatenate(batch6_)\n",
        "      flat = keras.layers.Flatten()(concat)\n",
        "      dim = np.round(self.d * batch6_[i].shape[1] * batch6_[i].shape[2])\n",
        "    else:  # only deep mode\n",
        "      input_ = keras.layers.Input(shape=x.shape[1:])\n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "      conv_ = keras.layers.Conv2D(filters=self.n_fil, kernel_size=5, strides=1, activation='relu',\n",
        "                                  padding='SAME', input_shape=x.shape[1:],\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1, l2=self.l1),name='C1_')(input_)\n",
        "      batch1_ = tf.keras.layers.BatchNormalization()(conv_)\n",
        "      pool_ = keras.layers.MaxPooling2D(pool_size=2)(batch1_)\n",
        "      batch2_ = tf.keras.layers.BatchNormalization()(pool_)\n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "      conv1_ = keras.layers.Conv2D(filters=self.n_fil, kernel_size=5, strides=1, activation='relu',\n",
        "                                  padding='SAME', input_shape=x.shape[1:],\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1, l2=self.l1),name='C2_')(batch2_)\n",
        "      batch3_ = tf.keras.layers.BatchNormalization()(conv1_)\n",
        "      pool1_ = keras.layers.MaxPooling2D(pool_size=2)(batch3_)\n",
        "      batch4_ = tf.keras.layers.BatchNormalization()(pool1_)\n",
        "      \n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "      conv2_ = keras.layers.Conv2D(filters=self.n_fil, kernel_size=5, strides=1, activation='relu',\n",
        "                                  padding='SAME', input_shape=x.shape[1:],\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1, l2=self.l1),name='C3_')(batch4_)\n",
        "      batch5_ = tf.keras.layers.BatchNormalization()(conv2_)\n",
        "      pool2_ = keras.layers.MaxPooling2D(pool_size=2)(batch5_)\n",
        "      batch6_ = tf.keras.layers.BatchNormalization()(pool2_)\n",
        "      \n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "      flat = keras.layers.Flatten()(batch6_)\n",
        "      dim = np.round(self.d * flat.shape[1])\n",
        "    # multilayer dense  perceptron with rff\n",
        "    # -----------------------------------------------------------------------------------------\n",
        "    flat_do = tf.keras.layers.Dropout(rate=0.25)(flat)\n",
        "    d1 = tf.keras.layers.Dense(dim.astype('int'), activation='linear', kernel_initializer=\"GlorotNormal\",\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=self.l1, l2=self.l1))(flat_do)\n",
        "    h1 = tf.keras.layers.experimental.RandomFourierFeatures(output_dim=dim.astype('int'),\n",
        "                                                            scale=self.sigma, kernel_initializer='gaussian',\n",
        "                                                            trainable=True, name='rbf_fourier')(d1)\n",
        "    h1_bn = tf.keras.layers.BatchNormalization(name='brff')(h1)\n",
        "    output_prob = tf.keras.layers.Dense(self.k, activation='softmax', name='out', kernel_initializer=\"GlorotNormal\",\n",
        "                                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=self.l1, l2=self.l1))(\n",
        "        h1_bn)\n",
        "    # define loss and compile\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
        "    if self.sl == 'ritl':\n",
        "      model = tf.keras.Model(inputs=input_, outputs=[output_prob, h1_bn])\n",
        "      model_loss = [tf.keras.losses.CategoricalCrossentropy(), self.custom_ritl()]\n",
        "      model.compile(loss=model_loss, loss_weights=[self.lk, 1 - self.lk], optimizer=opt,\n",
        "                    metrics=['accuracy'])  # f1, precision, re\n",
        "    elif self.sl == 'ce':\n",
        "      model = tf.keras.Model(inputs=input_, outputs=output_prob)\n",
        "      model_loss = [tf.keras.losses.CategoricalCrossentropy()]\n",
        "      model.compile(loss=model_loss, optimizer=opt, metrics=['accuracy'])  # f1, precision, re\n",
        "    else:\n",
        "      model = tf.keras.Model(inputs=input_, outputs=output_prob)\n",
        "      model_loss = [tf.keras.losses.MeanSquaredError()]\n",
        "      model.compile(loss=model_loss, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    rs = ShuffleSplit(n_splits=1, test_size=.1)\n",
        "\n",
        "    # validation data\n",
        "    for train_index, valid_index in rs.split(x):\n",
        "      if self.wi:\n",
        "\n",
        "          x_pre = conv_to_list(x[train_index])\n",
        "          x_pre_v = conv_to_list(x[valid_index])\n",
        "\n",
        "      else:\n",
        "          x_pre = x[train_index]\n",
        "          x_pre_v = x[valid_index]\n",
        "\n",
        "    y_pre = keras.utils.to_categorical(y, self.k)  #\n",
        "\n",
        "    if self.sl == 'ritl':\n",
        "      history = model.fit(x_pre, [y_pre[train_index], y_pre[train_index]],\n",
        "                          epochs=self.epochs,\n",
        "                          batch_size=self.batch_size,\n",
        "                          validation_data=(x_pre_v, [y_pre[valid_index], y_pre[valid_index]]),\n",
        "                          verbose=self.verbose)\n",
        "    else:\n",
        "      history = model.fit(X_pre, y_pre[train_index],\n",
        "                          epochs=self.epochs,\n",
        "                          batch_size=self.batch_size,\n",
        "                          validation_data=(X_pre_v, y_pre[valid_index]),\n",
        "                          verbose=self.verbose)\n",
        "    if self.plot_model:\n",
        "      plot_model_(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "  def predict(self, x_test):\n",
        "\n",
        "    if self.wi:\n",
        "      x_test_pre = conv_to_list(x_test)\n",
        "    else:\n",
        "      x_test_pre = x_test\n",
        "\n",
        "    if self.sl == 'ritl':\n",
        "      y_prob = np.stack(\n",
        "          [self.model(x_test_pre, training=True)[0]\n",
        "            for sample in range(100)])\n",
        "    else:\n",
        "      y_prob = np.stack(\n",
        "          [self.model(x_test_pre, training=True)\n",
        "            for sample in range(100)])\n",
        "\n",
        "    y_prob = y_prob.mean(axis=0)\n",
        "    label_e = np.argmax(y_prob, axis=1)\n",
        "\n",
        "    return label_e\n",
        "\n",
        "  def custom_ritl(self):\n",
        "    def custom_kitl(y_true, y_pred):\n",
        "      # -------------------------kernel----------------------------\n",
        "      k = tf.matmul(y_pred, y_pred, transpose_b=True)\n",
        "      # ----------------------center-------------------------------\n",
        "      N = tf.cast(tf.shape(k)[0], dtype=tf.float32)\n",
        "      # matrix for centered kernel\n",
        "      h = tf.eye(N) - (1.0 / N) * tf.ones([N, 1]) * tf.ones([1, N])\n",
        "      k = tf.matmul(tf.matmul(k, h), tf.matmul(k, h))\n",
        "      k = tf.math.divide_no_nan(k, tf.linalg.trace(k))\n",
        "      # ------------------------F_initial--------------------------\n",
        "      f = -tf.math.log(tf.linalg.trace(tf.matmul(k, k) + 1E-10))\n",
        "      return -f\n",
        "\n",
        "    return custom_kitl\n",
        "\n",
        "  def get_params(self, deep=True):\n",
        "\n",
        "    return {'d': self.d, 'k': self.k, 'epochs': self.epochs, 'batch_size': self.batch_size,\n",
        "            'lr': self.lr, 'sl': self.sl, 'lk': self.lk, 'l1': self.l1,\n",
        "            'wi': self.wi, 'sbj': self.sbj, 'verbose': self.verbose, 'n_fil': self.n_fil,\n",
        "            'plot_model': self.plot_model}\n",
        "\n",
        "  def set_params(self, **parameters):\n",
        "    for parameter, value in parameters.items():\n",
        "      setattr(self, parameter, value)\n",
        "    return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jHt9ZvYmFYB"
      },
      "source": [
        "## Cargar sujetos para csp y cwt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53TqrVXuq7U3"
      },
      "source": [
        "try:\n",
        "  os.makedirs('/content/csp y cwt/Modelos/')\n",
        "except:\n",
        "  print('ya esxiste la carpeta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "uAB4aLeLmS-A",
        "outputId": "68fb0750-1ef2-4822-b9ae-46d8ff28d207"
      },
      "source": [
        "aux = np.arange(52)+1\n",
        "subjects = np.delete(aux,[28,33])\n",
        "for sbj in subjects:\n",
        "  print('Sujeto: ',sbj)  \n",
        "  Xcsp = np.load('/content/csp y cwt/csp/Fold1/train/sbj'+str(sbj)+'.npy')\n",
        "  Xcwt = np.load('/content/csp y cwt/cwt/Fold1/train/sbj'+str(sbj)+'.npy')\n",
        "  X_train = np.expand_dims(np.concatenate([np.moveaxis(Xcsp, -1, 3).reshape(Xcsp.shape[0], Xcsp.shape[1], Xcsp.shape[2], -1 ), np.moveaxis(Xcwt, -1, 3).reshape(Xcwt.shape[0], Xcwt.shape[1], Xcwt.shape[2], -1 )], axis=-1), axis=3)\n",
        "  y_train = np.load('/content/csp y cwt/Targets/Fold1/trainsbj'+str(sbj)+'.npy')\n",
        "\n",
        "  Xcsp = np.load('/content/csp y cwt/csp/Fold1/test/sbj'+str(sbj)+'.npy')\n",
        "  Xcwt = np.load('/content/csp y cwt/cwt/Fold1/test/sbj'+str(sbj)+'.npy')\n",
        "  X_test = np.expand_dims(np.concatenate([np.moveaxis(Xcsp, -1, 3).reshape(Xcsp.shape[0], Xcsp.shape[1], Xcsp.shape[2], -1 ), np.moveaxis(Xcwt, -1, 3).reshape(Xcwt.shape[0], Xcwt.shape[1], Xcwt.shape[2], -1 )], axis=-1), axis=3)\n",
        "  y_test = np.load('/content/csp y cwt/Targets/Fold1/testsbj'+str(sbj)+'.npy')\n",
        "\n",
        "  X = np.concatenate((X_train,X_test),axis=0)\n",
        "  y = np.concatenate((y_train, y_test))\n",
        "  #---------------------------------------------------------------------------------\n",
        "  # Correr solo el modelo\n",
        "  #P= CNNrITL(d=2, k=len(np.unique(y)), epochs=150, batch_size=64, \n",
        "  #           lr=1e-3, sl='ritl', lk=0.5, l1=1e-3, plot_model=True)\n",
        "  #P.fit(X,y)\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # Correr con grid search\n",
        "  steps = [('proy', CNNrITL2(epochs=150, batch_size=64, k=len(np.unique(y)), lr = 1e-3, sl='ritl')),\n",
        "           ]    \n",
        "  method = Pipeline(steps ,memory='datospipeline')\n",
        "  parameters ={ 'proy__l1':[1e-3, 1e-2, 1e-1],\n",
        "                'proy__lk':[0.5,0.75,1],\n",
        "                'proy__d':[2,5,10],\n",
        "                }\n",
        "  cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=123)\n",
        "  grid_search = GridSearchCV(method, parameters,cv=cv,verbose=10,\n",
        "                      scoring='accuracy',refit='accuracy',n_jobs=-1,error_score='raise' )\n",
        "  grid_search.fit(X, y)\n",
        "  print(grid_search.best_params_)\n",
        "  # guardar modelo \n",
        "  grid_search.best_estimator_[0].model.save('/content/csp y cwt/Modelos/sbj'+str(sbj)+'.h5')\n",
        "  print('para el sujeto {} el accuracy  es {}'.format(sbj,grid_search.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sujeto:  1\n",
            "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  6.5min\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 13.6min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 32.6min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 48.9min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed: 69.7min\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d15928c0c7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m   grid_search = GridSearchCV(method, parameters,cv=cv,verbose=10,\n\u001b[1;32m     33\u001b[0m                       scoring='accuracy',refit='accuracy',n_jobs=-1,error_score='raise' )\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m# guardar modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-jy8IccmLuB"
      },
      "source": [
        "## Cargar sujetos para gauss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wc0oDQ9rjhR"
      },
      "source": [
        "try:\n",
        "  os.makedirs('/content/gauss/Modelos/')\n",
        "except:\n",
        "  print('ya esxiste la carpeta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Norui06oe--z",
        "outputId": "f116b083-5098-456b-a58b-3e740bea4ca3"
      },
      "source": [
        "aux = np.arange(52)+1\n",
        "subjects = np.delete(aux,[28,33])\n",
        "for sbj in subjects:\n",
        "  print(sbj)\n",
        "  X = np.load('/content/gauss/Gaussian_Kernel/sbj'+str(sbj)+'.npy')\n",
        "  X = np.moveaxis(X, -1, 4).reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], -1)\n",
        "  y = np.load('/content/gauss/Targets/sbj'+str(sbj)+'.npy')\n",
        "  #---------------------------------------------------------------------------------\n",
        "  # Correr solo el modelo\n",
        "  P= CNNrITL(d=2, k=len(np.unique(y)), epochs=150, batch_size=64, \n",
        "             lr=1e-3, sl='ritl', lk=0.5, l1=1e-3, plot_model=True)\n",
        "  P.fit(X,y)\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # Correr con grid search\n",
        "  steps = [('proy', CNNrITL(epochs=150, batch_size=64, k=len(np.unique(y)), lr = 1e-3, sl='ritl')),\n",
        "           ]    \n",
        "  method = Pipeline(steps ,memory='datospipeline')\n",
        "  parameters ={ 'proy__l1':[1e-3, 1e-2, 1e-1],\n",
        "                'proy__lk':[0.5,0.75,1],\n",
        "                'proy__d':[2,5,10],\n",
        "                }\n",
        "  cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=123)\n",
        "  grid_search = GridSearchCV(method, parameters,cv=cv,verbose=10,\n",
        "                      scoring='accuracy',refit='accuracy',n_jobs=-1,error_score='raise' )\n",
        "  grid_search.fit(X, y)\n",
        "  print(grid_search.best_params_)\n",
        "  # guardar modelo \n",
        "  grid_search.best_estimator_[0].model.save('/content/gauss/Modelos/sbj'+str(sbj)+'.h5')\n",
        "  print('para el sujeto {} el accuracy  es {}'.format(sbj,grid_search.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5OEMpFVm-Qo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}