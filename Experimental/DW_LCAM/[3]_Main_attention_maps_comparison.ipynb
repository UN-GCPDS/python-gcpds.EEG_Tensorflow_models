{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[3]_Main_attention_maps_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UN-GCPDS/python-gcpds.EEG_Tensorflow_models/blob/main/Experimental/DW_LCAM/%5B3%5D_Main_attention_maps_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3756n3xnQh"
      },
      "source": [
        "# Load drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUjvvh9Vxm-p"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plu4I_-dMorV"
      },
      "source": [
        "# Install Keras-vis toolbox\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCSY1t6J8s8"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "!pip install tf-keras-vis tensorflow\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQrwoiJbxqTY"
      },
      "source": [
        "# Supporting modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qePz_s64pqIj"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from matplotlib import cm\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras import backend as K\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.scorecam import Scorecam\n",
        "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.metrics import pairwise_distances\n",
        "%matplotlib inline\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJXpdxGvxazY"
      },
      "source": [
        "# Define load data, normalization and CNN model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emU_mxydxafh"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def TW_data(sbj,time_inf,time_sup):\n",
        "    # Load data/images----------------------------------------------------------\n",
        "    path_cwt = '/content/drive/MyDrive/Colab Notebooks/GradCam_Paper/GigaData/data/CWT_CSP_data_mubeta_8_30_Tw_'+str(time_inf)+'s_'+str(time_sup)+'s_subject'+str(sbj)+'_cwt_resized_10.pickle'  \n",
        "    with open(path_cwt, 'rb') as f:\n",
        "         X_train_re_cwt, X_test_re_cwt, y_train, y_test = pickle.load(f)\n",
        "    path_csp = '/content/drive/MyDrive/Colab Notebooks/GradCam_Paper/GigaData/data/CWT_CSP_data_mubeta_8_30_Tw_'+str(time_inf)+'s_'+str(time_sup)+'s_subject'+str(sbj)+'_csp_resized_10.pickle'  \n",
        "    with open(path_csp, 'rb') as f:\n",
        "         X_train_re_csp, X_test_re_csp, y_train, y_test = pickle.load(f)\n",
        "    #---------------------------------------------------------------------------\n",
        "    return X_train_re_cwt, X_train_re_csp, X_test_re_cwt, X_test_re_csp, y_train, y_test\n",
        "#-------------------------------------------------------------------------------\n",
        "def norm_data(XF_train_cwt, XF_train_csp, XF_test_cwt, XF_test_csp, n_fb, Ntw, y_train, y_test, fld):\n",
        "    # orden de las inputs:------------------------------------------------------\n",
        "    # [CWT_fb1_TW1, CWT_fb2_TW1 --- CWT_fb1_TW2, CWT_fb2_TW2 --- CWT_fb1_TWN, CWT_fb2_TWN] ... [CSP]\n",
        "    #---------------------------------------------------------------------------\n",
        "    XT_train_csp = []\n",
        "    XT_valid_csp = []\n",
        "    XT_test_csp  = []\n",
        "    XT_train_cwt = []\n",
        "    XT_valid_cwt = []\n",
        "    XT_test_cwt  = []\n",
        "    for tw in range(Ntw):\n",
        "        for fb in range(n_fb):\n",
        "            X_train_cwt, X_test_cwt = XF_train_cwt[tw][:,fb,:,:].astype(np.uint8), XF_test_cwt[tw][:,fb,:,:].astype(np.uint8)\n",
        "            X_train_csp, X_test_csp = XF_train_csp[tw][:,fb,:,:].astype(np.uint8), XF_test_csp[tw][:,fb,:,:].astype(np.uint8)\n",
        "            #-------------------------------------------------------------------\n",
        "            # train/validation data split\n",
        "            rs = ShuffleSplit(n_splits=1, test_size=.1, random_state=fld)\n",
        "            for train_index, valid_index in rs.split(X_train_cwt):\n",
        "              X_train_cwtf = X_train_cwt[train_index,:,:] # cwt\n",
        "              X_valid_cwtf = X_train_cwt[valid_index,:,:]\n",
        "              X_train_cspf = X_train_csp[train_index,:,:] # csp\n",
        "              X_valid_cspf = X_train_csp[valid_index,:,:]\n",
        "            #-------------------------------------------------------------------          \n",
        "            # Normalize data----------------------------------------------------\n",
        "            X_mean_cwt  = X_train_cwtf.mean(axis=0, keepdims=True)\n",
        "            X_std_cwt   = X_train_cwtf.std(axis=0, keepdims=True) + 1e-7\n",
        "            X_train_cwt = (X_train_cwtf - X_mean_cwt) / X_std_cwt\n",
        "            X_valid_cwt = (X_valid_cwtf - X_mean_cwt) / X_std_cwt\n",
        "            X_test_cwt  = (X_test_cwt  - X_mean_cwt) / X_std_cwt\n",
        "\n",
        "            X_mean_csp  = X_train_cspf.mean(axis=0, keepdims=True)\n",
        "            X_std_csp   = X_train_cspf.std(axis=0, keepdims=True) + 1e-7\n",
        "            X_train_csp = (X_train_cspf - X_mean_csp) / X_std_csp\n",
        "            X_valid_csp = (X_valid_cspf - X_mean_csp) / X_std_csp\n",
        "            X_test_csp  = (X_test_csp  - X_mean_csp) / X_std_csp\n",
        "            #-------------------------------------------------------------------\n",
        "            # set new axis------------------------------------------------------\n",
        "            X_train_cwt = X_train_cwt[..., np.newaxis]\n",
        "            X_valid_cwt = X_valid_cwt[..., np.newaxis]\n",
        "            X_test_cwt  = X_test_cwt[..., np.newaxis]   \n",
        "            XT_train_cwt.append(X_train_cwt)\n",
        "            XT_valid_cwt.append(X_valid_cwt)\n",
        "            XT_test_cwt.append(X_test_cwt)\n",
        "                                \n",
        "            X_train_csp = X_train_csp[..., np.newaxis]\n",
        "            X_valid_csp = X_valid_csp[..., np.newaxis]\n",
        "            X_test_csp  = X_test_csp[..., np.newaxis]   \n",
        "            XT_train_csp.append(X_train_csp)\n",
        "            XT_valid_csp.append(X_valid_csp)\n",
        "            XT_test_csp.append(X_test_csp)\n",
        "            #-------------------------------------------------------------------\n",
        "    y_trainf = y_train[train_index]\n",
        "    y_validf = y_train[valid_index]\n",
        "    y_trainF, y_validF, y_testF = y_trainf.reshape((-1,))-1, y_validf.reshape((-1,))-1, y_test.reshape((-1,))-1\n",
        "    #---------------------------------------------------------------------------\n",
        "    # Convert class vectors to binary class matrices----------------------------\n",
        "    y_train = keras.utils.to_categorical(y_trainF,num_classes)\n",
        "    y_valid = keras.utils.to_categorical(y_validF,num_classes)\n",
        "    y_test  = keras.utils.to_categorical(y_testF,num_classes)\n",
        "    #---------------------------------------------------------------------------\n",
        "    XT_train = XT_train_cwt + XT_train_csp\n",
        "    XT_valid = XT_valid_cwt + XT_valid_csp\n",
        "    XT_test  = XT_test_cwt  + XT_test_csp\n",
        "    #---------------------------------------------------------------------------\n",
        "    return XT_train, XT_valid, XT_test, y_train, y_valid, y_test, train_index, valid_index  \n",
        "#-------------------------------------------------------------------------------\n",
        "def vis_heatmap(HmapT,Ntw,names_x,norm):\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # normalizing heatmap\n",
        "  if norm == 1:\n",
        "    hmap_max = np.max(np.array(HmapT))\n",
        "    for i in range(20):\n",
        "      HmapT[i] = tf.math.divide_no_nan(HmapT[i],hmap_max)\n",
        "    new_max = np.max(np.array(HmapT))\n",
        "    new_min = np.min(np.array(HmapT))\n",
        "  else:\n",
        "    for i in range(20):\n",
        "      print(np.max(np.array(HmapT[i])),np.min(np.array(HmapT[i])))\n",
        "      HmapT[i] = tf.math.divide_no_nan(HmapT[i],np.max(np.array(HmapT[i])))\n",
        "    new_max = np.max(np.array(HmapT))\n",
        "    new_min = np.min(np.array(HmapT))\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # figure plot setting\n",
        "  fig, axs = plt.subplots(4,5,figsize=(12,7.3))\n",
        "  fig.subplots_adjust(hspace = 0.1, wspace=.0001)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # creating figure\n",
        "  for tw in range(Ntw):\n",
        "      if tw == 0:\n",
        "        ids_tw = [tw, tw+1, tw+10, tw+10+1] \n",
        "      else:\n",
        "        ids_tw = [tw*2, tw*2+1, tw*2+10, tw*2+10+1]\n",
        "      axs[0,tw].matshow(HmapT[ids_tw[0]],vmin=new_min, vmax=new_max)\n",
        "      axs[1,tw].matshow(HmapT[ids_tw[1]],vmin=new_min, vmax=new_max)\n",
        "      axs[2,tw].matshow(HmapT[ids_tw[2]],vmin=new_min, vmax=new_max)\n",
        "      axs[3,tw].matshow(HmapT[ids_tw[3]],vmin=new_min, vmax=new_max)\n",
        "      axs[3,tw].set(xlabel=names_x[tw])\n",
        "      axs[3,tw].xaxis.get_label().set_fontsize(15)\n",
        "      if tw == 0:\n",
        "        axs[0,tw].set(ylabel=r'$CWT \\mu$')\n",
        "        axs[0,tw].yaxis.get_label().set_fontsize(15)\n",
        "        axs[1,tw].set(ylabel=r'$CWT \\beta$')\n",
        "        axs[1,tw].yaxis.get_label().set_fontsize(15)\n",
        "        axs[2,tw].set(ylabel=r'$CSP \\mu$')\n",
        "        axs[2,tw].yaxis.get_label().set_fontsize(15)\n",
        "        axs[3,tw].set(ylabel=r'$CSP \\beta$')\n",
        "        axs[3,tw].yaxis.get_label().set_fontsize(15)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  for ax in axs.flat:\n",
        "      ax.label_outer()\n",
        "  for ax in axs.flat:\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "#-------------------------------------------------------------------------------\n",
        "def vis_render(HmapT,new_input,Ntw):\n",
        "  f, ax = plt.subplots(nrows=4, ncols=5, figsize=(12,7.3))\n",
        "  f.subplots_adjust(hspace = 0.1, wspace=.0001)\n",
        "  for tw in range(Ntw):\n",
        "      if tw == 0:\n",
        "        ids_tw = [tw, tw+1, tw+10, tw+10+1] \n",
        "      else:\n",
        "        ids_tw = [tw*2, tw*2+1, tw*2+10, tw*2+10+1]\n",
        "      heatmap_0 = np.uint8(cm.jet(HmapT[ids_tw[0]])[..., :3] * 255)\n",
        "      heatmap_1 = np.uint8(cm.jet(HmapT[ids_tw[1]])[..., :3] * 255)\n",
        "      heatmap_2 = np.uint8(cm.jet(HmapT[ids_tw[2]])[..., :3] * 255)\n",
        "      heatmap_3 = np.uint8(cm.jet(HmapT[ids_tw[3]])[..., :3] * 255)\n",
        "      \n",
        "      ax[0,tw].imshow(np.squeeze(new_input[ids_tw[0]]), cmap='gray',vmin=0,vmax=1)\n",
        "      ax[1,tw].imshow(np.squeeze(new_input[ids_tw[1]]), cmap='gray',vmin=0,vmax=1)\n",
        "      ax[2,tw].imshow(np.squeeze(new_input[ids_tw[2]]), cmap='gray',vmin=0,vmax=1)\n",
        "      ax[3,tw].imshow(np.squeeze(new_input[ids_tw[3]]), cmap='gray',vmin=0,vmax=1)\n",
        "      ax[0,tw].imshow(heatmap_0, cmap='jet', alpha=0.5) # overlay\n",
        "      ax[1,tw].imshow(heatmap_1, cmap='jet', alpha=0.5) # overlay\n",
        "      ax[2,tw].imshow(heatmap_2, cmap='jet', alpha=0.5) # overlay\n",
        "      ax[3,tw].imshow(heatmap_3, cmap='jet', alpha=0.5) # overlay\n",
        "      if tw == 0:\n",
        "        ax[0,tw].set(ylabel=r'$CWT \\mu$')\n",
        "        ax[0,tw].yaxis.get_label().set_fontsize(15)\n",
        "        ax[1,tw].set(ylabel=r'$CWT \\beta$')\n",
        "        ax[1,tw].yaxis.get_label().set_fontsize(15)\n",
        "        ax[2,tw].set(ylabel=r'$CSP \\mu$')\n",
        "        ax[2,tw].yaxis.get_label().set_fontsize(15)\n",
        "        ax[3,tw].set(ylabel=r'$CSP \\beta$')\n",
        "        ax[3,tw].yaxis.get_label().set_fontsize(15)\n",
        "  for ax in ax.flat:\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "#-------------------------------------------------------------------------------\n",
        "def cnn_network(n_fb,Nkfeats,Ntw,shape_,n_filt,units,l1p,l2p,lrate,sbj):\n",
        "    #---------------------------------------------------------------------------\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(123)\n",
        "    tf.compat.v1.random.set_random_seed(123)\n",
        "    #---------------------------------------------------------------------------\n",
        "    input_  = [None]*Ntw*n_fb*Nkfeats\n",
        "    conv_   = [None]*Ntw*n_fb*Nkfeats\n",
        "    pool_   = [None]*Ntw*n_fb*Nkfeats\n",
        "    batch0_ = [None]*Ntw*n_fb*Nkfeats\n",
        "    batch2_ = [None]*Ntw*n_fb*Nkfeats\n",
        "    for i in range(Ntw*n_fb*Nkfeats):\n",
        "        input_[i]  = keras.layers.Input(shape=[shape_,shape_,1])\n",
        "        conv_[i]   = keras.layers.Conv2D(filters=n_filt,kernel_size=3,strides=1,activation='relu',padding='SAME',input_shape=[shape_,shape_,1])(input_[i])\n",
        "        #-----------------------------------------------------------------------\n",
        "        batch0_[i] = keras.layers.BatchNormalization()(conv_[i])\n",
        "        #-----------------------------------------------------------------------\n",
        "        pool_[i]   = keras.layers.MaxPooling2D(pool_size=2)(batch0_[i])\n",
        "        #-----------------------------------------------------------------------\n",
        "    concat  = keras.layers.concatenate(pool_)\n",
        "    flat    = keras.layers.Flatten()(concat)\n",
        "    #---------------------------------------------------------------------------\n",
        "    batch1  = keras.layers.BatchNormalization()(flat)\n",
        "    hidden1 = keras.layers.Dense(units=units,activation='relu',kernel_regularizer=keras.regularizers.l1_l2(l1=l1p, l2=l2p), kernel_constraint=max_norm(1.))(batch1)#\n",
        "    batch2  = keras.layers.BatchNormalization()(hidden1)\n",
        "    output  = keras.layers.Dense(units=2, activation='softmax', kernel_constraint=max_norm(1.))(batch2)#\n",
        "    model   = keras.models.Model(inputs=input_, outputs=[output])\n",
        "    #---------------------------------------------------------------------------\n",
        "    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(lrate, 4000, power=1.0,cycle=False, name=None)\n",
        "    opt     = keras.optimizers.Adam(learning_rate=learning_rate_fn) \n",
        "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDpoua7-x7lf"
      },
      "source": [
        "# Perform GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S66cx-qLHdQ9"
      },
      "source": [
        "#attention maps wide models\n",
        "#attention maps\n",
        "\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "def centroid_(X):\n",
        "   D = pairwise_distances(X, X.mean(axis=0).reshape(1,-1))\n",
        "   inertia_ = D.mean()\n",
        "   return np.argmin(D),inertia_\n",
        "\n",
        "\n",
        "\n",
        "def plot_attention(tmpr_,rel_model_name,layer_name,list_class,figsize=(10,5), transpose=False):\n",
        "    \n",
        "    names_feats = [r'CWT-$\\mu$-TW1',r'CWT-$\\beta$-TW1',r'CWT-$\\mu$-TW2',r'CWT-$\\beta$-TW2',r'CWT-$\\mu$-TW3',r'CWT-$\\beta$-TW3',r'CWT-$\\mu$-TW4',r'CWT-$\\beta$-TW4',r'CWT-$\\mu$-TW5',r'CWT-$\\beta$-TW5',\n",
        "                   r'CSP-$\\mu$-TW1',r'CSP-$\\beta$-TW1',r'CSP-$\\mu$-TW2',r'CSP-$\\beta$-TW2',r'CSP-$\\mu$-TW3',r'CSP-$\\beta$-TW3',r'CSP-$\\mu$-TW4',r'CSP-$\\beta$-TW4',r'CSP-$\\mu$-TW5',r'CSP-$\\beta$-TW5']\n",
        "    if transpose:\n",
        "      x_label_list = layer_name \n",
        "      nC = len(list_class)\n",
        "      nl = len(layer_name)\n",
        "      ncols,nrows = tmpr_.shape\n",
        "\n",
        "      y_label_list = []\n",
        "      for ii in range(nC):\n",
        "          y_label_list += str(list_class[ii])\n",
        "\n",
        "      dw = nrows/nl\n",
        "      list_xticks = []\n",
        "      for ii in range(nl):\n",
        "        list_xticks += [int(dw*(0.5+ii))]\n",
        "      dw = ncols/nC\n",
        "      list_yticks = []\n",
        "      for ii in range(nC):\n",
        "        list_yticks += [int(dw*(0.5+ii))]\n",
        "\n",
        "    else:\n",
        "      y_label_list = layer_name \n",
        "      nC = len(list_class)\n",
        "      nl = len(layer_name)\n",
        "      nrows,ncols = tmpr_.shape\n",
        "\n",
        "      x_label_list = []\n",
        "      for ii in range(nC):\n",
        "          x_label_list += str(list_class[ii])\n",
        "\n",
        "      dw = nrows/nl\n",
        "      list_yticks = []\n",
        "      for ii in range(nl):\n",
        "        list_yticks += [int(dw*(0.5+ii))]\n",
        "      dw = ncols/nC\n",
        "      list_xticks = []\n",
        "      for ii in range(nC):\n",
        "        list_xticks += [int(dw*(0.5+ii))]\n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    ax = plt.gca()\n",
        "    im = ax.imshow(tmpr_)\n",
        "    im = ax.imshow(tmpr_)\n",
        "    ax.set_yticks(list_yticks)\n",
        "    ax.set_yticklabels(y_label_list)\n",
        "    ax.set_xticks(list_xticks)\n",
        "    ax.set_xticklabels(names_feats, rotation='vertical')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "        \n",
        "    plt.colorbar(im, cax=cax,extend='both',\n",
        "                 ticks=[np.round(tmpr_.min(),3), np.round(0.5*(tmpr_.max()-tmpr_.min()),3), np.round(tmpr_.max(),3)])\n",
        "    plt.xticks(rotation=90)\n",
        "    #plt.savefig('/content/drive/MyDrive/Colab Notebooks/GradCam_Paper/GigaData/results/resulting_attention_maps/attention_map_'+str(n_sbj[sbj])+'_'+rel_model_name+'.svg', format='svg')\n",
        "       \n",
        "    plt.tight_layout()    \n",
        "    plt.show()\n",
        "\n",
        "import cv2\n",
        "def attention_wide(modelw,rel_model_name,layer_name,X_train,y_train,\n",
        "                   normalize_cam=False,norm_max_min=False,norm_c=True,\n",
        "                   plot_int=False,centroid_=False,smooth_samples=20,\n",
        "                   smooth_noise=0.20,transpose=False):\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # define trial sample to visualize\n",
        "    # change activations of last layer by linear\n",
        "    replace2linear = ReplaceToLinear()\n",
        "    #relevance model\n",
        "    \n",
        "    if rel_model_name == 'Weights':\n",
        "      #[topo_avg_muT_cwt,topo_avg_beT_cwt,topo_avg_muT_csp,topo_avg_beT_csp]\n",
        "      path='/content/drive/MyDrive/Colab Notebooks/GradCam_Paper/GigaData/results/matrix_data/WeightsRel_sbj_'+str(n_sbj[sbj])+'_fold_'+str(opt_fld[sbj])+'.pickle'\n",
        "      with open(path, 'rb') as f:\n",
        "         w_data = pickle.load(f)\n",
        "      \n",
        "      for i in range(5):\n",
        "        if i ==0:\n",
        "          amw_cwt = cv2.resize(w_data[0][i,:,:],(40, 40),interpolation = cv2.INTER_NEAREST)\n",
        "        else: \n",
        "          amw_cwt = np.c_[amw_cwt,cv2.resize(w_data[0][i,:,:],(40, 40),interpolation = cv2.INTER_NEAREST)]\n",
        "        amw_cwt = np.c_[amw_cwt,cv2.resize(w_data[1][i,:,:],(40, 40),interpolation = cv2.INTER_NEAREST)]\n",
        "      \n",
        "      for i in range(5):\n",
        "        if i ==0:\n",
        "          amw_csp = cv2.resize(w_data[2][i,:,:],(40, 40),interpolation = cv2.INTER_NEAREST)\n",
        "        else: \n",
        "          amw_csp = np.c_[amw_csp,cv2.resize(w_data[2][i,:,:],(40, 40),interpolation = cv2.INTER_NEAREST)]\n",
        "        amw_csp   = np.c_[amw_csp,cv2.resize(w_data[3][i,:,:],(40, 40),interpolation = cv2.INTER_NEAREST)]\n",
        "  \n",
        "      amw = np.concatenate((amw_cwt,amw_csp),axis=1)\n",
        "      amw = np.r_[amw,amw]\n",
        "      relM = [None]*len(np.unique(y_train))\n",
        "      #---------------------------------------------------------------------------\n",
        "      tmpr = amw/(1e-8+amw.max())\n",
        "      #---------------------------------------------------------------------------\n",
        "\n",
        "    else:\n",
        "      if rel_model_name == 'Gradcam':\n",
        "          gradcamw = Gradcam(modelw,\n",
        "                          model_modifier=replace2linear,\n",
        "                          clone=True)\n",
        "      elif rel_model_name == 'Gradcam++':\n",
        "          gradcamw = GradcamPlusPlus(modelw,\n",
        "                                model_modifier=replace2linear,\n",
        "                                clone=True) \n",
        "          \n",
        "      elif rel_model_name == 'Scorecam':\n",
        "          scorecamw = Scorecam(modelw)\n",
        "          \n",
        "      elif rel_model_name == 'Saliency':\n",
        "            saliencyw = Saliency(modelw,\n",
        "                                model_modifier=replace2linear,\n",
        "                                clone=True)\n",
        "            layer_name = [''] #saliency doesn't depend on different layers    \n",
        "      nC = len(np.unique(y_train))\n",
        "      relM = [None]*nC\n",
        "      if type(X_train)==list:\n",
        "          n_inputs = len(X_train)\n",
        "          new_input = [None]*n_inputs\n",
        "\n",
        "      for c in range(len(np.unique(y_train))):  \n",
        "        id_sample = y_train == np.unique(y_train)[c]\n",
        "\n",
        "        if (type(X_train)==list) and (rel_model_name != 'Saliency'):\n",
        "          relM[c] = np.zeros((sum(id_sample),X_train[0].shape[1],X_train[0].shape[2],len(layer_name)))\n",
        "          #print(1,relM[c].shape)\n",
        "        elif (type(X_train)==list) and (rel_model_name == 'Saliency'):   \n",
        "          relM[c] = np.zeros((sum(id_sample),X_train[0].shape[1],X_train[0].shape[2],len(X_train)))\n",
        "          #print(2,relM[c].shape)        \n",
        "        else:\n",
        "          relM[c] = np.zeros((sum(id_sample),X_train.shape[1],X_train.shape[2],len(layer_name)))\n",
        "          #print(3,relM[c].shape)\n",
        "        score = CategoricalScore(list(y_train[id_sample])) #-> [0] para probar a una clase diferente\n",
        "        if type(X_train)==list:\n",
        "            for ni in range(n_inputs):\n",
        "                new_input[ni] = X_train[ni][id_sample]\n",
        "        else:\n",
        "          new_input = X_train[id_sample]        \n",
        "        #print('rel',rel_model_name,'layer',layer_name[l])\n",
        "        for l in range(len(layer_name)):\n",
        "            #print(rel_model_name,'class', np.unique(y_train)[c],'layer',layer_name[l])\n",
        "        # label score -> target label accoring to the database\n",
        "        #-----------------------------------------------------------------------------\n",
        "        # generate heatmap with GradCAM\n",
        "            if (rel_model_name == 'Gradcam') or (rel_model_name == 'Gradcam++'):\n",
        "                rel = gradcamw(score,\n",
        "                            new_input,\n",
        "                            penultimate_layer=layer_name[l], #layer to be analized\n",
        "                            expand_cam=True,\n",
        "                            normalize_cam=normalize_cam)\n",
        "            elif rel_model_name == 'Saliency': #saliency map is too noisy, so let’s remove noise in the saliency map using SmoothGrad!\n",
        "                  rel = saliencyw(score, new_input,smooth_samples=smooth_samples,\n",
        "                                  smooth_noise=smooth_noise,\n",
        "                                  normalize_map=normalize_cam) #, smooth_samples=20,smooth_noise=0.20) # The number of calculating gradients iterations.\n",
        "                              \n",
        "            elif rel_model_name == 'Scorecam':     \n",
        "                rel = scorecamw(score, new_input, penultimate_layer=layer_name[l], #layer to be analized\n",
        "                            expand_cam=True,\n",
        "                            normalize_cam=normalize_cam) #max_N=10 -> faster scorecam\n",
        "        \n",
        "            #save model\n",
        "\n",
        "            if rel_model_name != 'Saliency':\n",
        "              if type(X_train)==list: \n",
        "                tcc = rel[0]\n",
        "              else: \n",
        "                tcc = rel\n",
        "              dimc = tcc.shape\n",
        "              tccv = tcc.ravel()\n",
        "              tccv[np.isnan(tccv)] = 0\n",
        "              tcc = tccv.reshape(dimc)\n",
        "              if norm_max_min: #normalizing along samples\n",
        "                tcc = MinMaxScaler().fit_transform(tcc.reshape(dimc[0],-1).T).T\n",
        "                tcc = tcc.reshape(dimc)\n",
        "              relM[c][...,l] = tcc\n",
        "              if l==0: \n",
        "                tmp = np.median(relM[c][...,l],axis=0)#relM[c][...,l].mean(axis=0)\n",
        "              else: \n",
        "                if transpose:\n",
        "                  tmp = np.c_[tmp,np.median(relM[c][...,l],axis=0)]#np.r_[tmp,relM[c][...,l].mean(axis=0)]  #centroid\n",
        "                else:  \n",
        "                  tmp = np.r_[tmp,np.median(relM[c][...,l],axis=0)]#np.r_[tmp,relM[c][...,l].mean(axis=0)]  #centroid\n",
        "            else: #saliency\n",
        "              if type(X_train)==list: \n",
        "                tcc = np.zeros((rel[0].shape[0],rel[0].shape[1],rel[0].shape[2],len(rel)))\n",
        "                for ii in range(len(rel)):\n",
        "                    tcc[...,ii] = rel[ii]\n",
        "              else: \n",
        "                tcc = rel\n",
        "              dimc = tcc.shape\n",
        "              tccv = tcc.ravel()\n",
        "              tccv[np.isnan(tccv)] = 0\n",
        "              tcc = tccv.reshape(dimc)\n",
        "              if norm_max_min: #normalizing along samples\n",
        "                tcc = MinMaxScaler().fit_transform(tcc.reshape(dimc[0],-1).T).T\n",
        "                tcc = tcc.reshape(dimc)\n",
        "              relM[c] = tcc\n",
        "              if type(X_train)==list: \n",
        "                tmp = np.median(tcc[...,0],axis=0)\n",
        "                for ii in range(len(rel)-1):\n",
        "                    if transpose: \n",
        "                      tmp = np.c_[tmp,np.median(tcc[...,ii+1],axis=0)]\n",
        "                    else:\n",
        "                      tmp = np.r_[tmp,np.median(tcc[...,ii+1],axis=0)]\n",
        "              else:\n",
        "                tmp = np.median(tcc,axis=0)\n",
        "                  \n",
        "        if norm_c: #normalizing along layers\n",
        "          tmp = tmp/(1e-8+tmp.max())\n",
        "        if c==0: \n",
        "          tmpr = tmp\n",
        "        else:  \n",
        "          if transpose:\n",
        "            tmpr = np.r_[tmpr,tmp]  \n",
        "          else:\n",
        "            tmpr = np.c_[tmpr,tmp]  \n",
        "        #print(tmp.shape,tmp.max())    \n",
        "        if plot_int: #plot every class\n",
        "          plt.imshow(tmp)\n",
        "          plt.colorbar(orientation='horizontal')\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "      #---------------------------------------------------------------------------\n",
        "      tmpr = tmpr/(1e-8+tmpr.max())\n",
        "      #---------------------------------------------------------------------------\n",
        "    \n",
        "    list_class = np.unique(y_train)\n",
        "    plot_attention(tmpr,rel_model_name,layer_name,list_class,transpose=transpose)\n",
        "\n",
        "    return relM,tmpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE35xUY6sVw8"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# define parameters\n",
        "partitions    = ['train','valid','test']\n",
        "names_x       = [r'-1.5s-0.5s',r'$-0.5s-1.5s$',r'$0.5s-2.5s$',r'$1.5s-3.5s$',r'$2.5s-4.5s$']\n",
        "learning_rate = 1e-4 \n",
        "th_name       = np.array([[-1.5, 0.5],[-0.5, 1.5],[0.5, 2.5],[1.5, 3.5],[2.5, 4.5]]) \n",
        "n_fb          = 2\n",
        "Ntw           = 5                             \n",
        "Nkfeats       = 2\n",
        "num_classes   = 2                              \n",
        "n_filt        = 2 \n",
        "n_fld         = 3\n",
        "n_conv_layers = 20\n",
        "#-------------------------------------------------------------------------------       \n",
        "n_sbj         = [41]#[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,30,31,32,33,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52]\n",
        "opt_neurons   = [200]#[100,200,100,300,200,100,300,200,100,100,200,200,200,300,300,100,100,100,300,200,300,300,200,100,100,300,200,300,300,200,300,200,300,300,100,200,300,300,200,100,200,200,100,300,300,100,100,300,100,300]\n",
        "opt_l1        = [0.005]#[0.0005,0.0005,0.005,0.005,0.001,0.001,0.0005,0.0005,0.0005,0.005,0.005,0.005,0.005,0.005,0.005,0.0005,0.0005,0.001,0.0005,0.0005,0.0005,0.0005,0.005,0.001,0.001,0.005,0.005,0.0005,0.0005,0.001,0.005,0.001,0.001,0.005,0.005,0.001,0.005,0.005,0.005,0.001,0.005,0.0005,0.005,0.005,0.0005,0.005,0.0005,0.005,0.005,0.005]\n",
        "opt_l2        = [0.0005]#[0.005,0.001,0.0005,0.005,0.005,0.001,0.0005,0.005,0.005,0.001,0.005,0.005,0.001,0.005,0.001,0.001,0.005,0.0005,0.0005,0.0005,0.0005,0.005,0.0005,0.005,0.005,0.0005,0.001,0.0005,0.0005,0.005,0.0005,0.005,0.0005,0.005,0.005,0.001,0.0005,0.001,0.0005,0.005,0.001,0.0005,0.001,0.0005,0.005,0.001,0.001,0.005,0.0005,0.001]\n",
        "opt_fld       = [1]#[3,1,1,3,3,1,2,2,3,2,1,2,1,2,1,3,2,1,2,1,1,1,3,1,1,2,1,3,3,1,1,2,2,1,1,3,2,1,1,3,2,2,3,1,2,1,1,3,1,1]\n",
        "#-------------------------------------------------------------------------------\n",
        "for sbj in range(len(n_sbj)):\n",
        "  print('subject ', n_sbj[sbj])\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # load data train/test trough all tw\n",
        "  XF_train_cwt = []\n",
        "  XF_train_csp = []\n",
        "  XF_test_cwt  = []\n",
        "  XF_test_csp  = []\n",
        "  for i in range(th_name.shape[0]):\n",
        "    X_train_re_cwt, X_train_re_csp, X_test_re_cwt, X_test_re_csp, y_trainF, y_testF = TW_data(n_sbj[sbj],th_name[i,0],th_name[i,1])\n",
        "    XF_train_cwt.append(X_train_re_cwt)\n",
        "    XF_train_csp.append(X_train_re_csp)\n",
        "    XF_test_cwt.append(X_test_re_cwt)\n",
        "    XF_test_csp.append(X_test_re_csp)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # partition of data\n",
        "  XT_train, XT_valid, XT_test, y_train, y_valid, y_test, train_index, valid_index = norm_data(XF_train_cwt, XF_train_csp, XF_test_cwt, XF_test_csp, n_fb, Ntw, y_trainF, y_testF, opt_fld[sbj]-1)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # define model\n",
        "  model = cnn_network(n_fb,Nkfeats,Ntw,40,n_filt,opt_neurons[sbj],opt_l1[sbj],opt_l2[sbj],learning_rate,n_sbj[sbj])\n",
        "  #-----------------------------------------------------------------------------\n",
        "  tf.keras.utils.plot_model(model)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # loading best model weights\n",
        "  filepath        = '/content/drive/MyDrive/Colab Notebooks/GradCam_Paper/GigaData/results/parameter_setting/weights_sbj_'+str(n_sbj[sbj])+'_filters_2_units_'+str(int(opt_neurons[sbj]))+'_l1_'+str(opt_l1[sbj])+'_l2_'+str(opt_l2[sbj])+'_fld_'+str(opt_fld[sbj])+'.hdf5'\n",
        "  checkpoint_path = filepath\n",
        "  model.load_weights(checkpoint_path)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  rel_model_name = ['Gradcam++','Scorecam','Saliency'] #,'Gradcam++','Scorecam','Saliency'\n",
        "  layer_name     = ['conv2d','conv2d_1','conv2d_2','conv2d_3','conv2d_4','conv2d_5','conv2d_6','conv2d_7','conv2d_8','conv2d_9','conv2d_10',\n",
        "                   'conv2d_11','conv2d_12','conv2d_13','conv2d_14','conv2d_15','conv2d_16','conv2d_17','conv2d_18','conv2d_19']\n",
        "  # \n",
        "  print('norm_c = False')\n",
        "  relM_ = [None]*len(rel_model_name) #relM[m] -> number classes x input image resolution x number of layers \n",
        "  tmpr_ = [None]*len(rel_model_name) \n",
        "  for m in range(len(rel_model_name)):\n",
        "    relM_[m],tmpr_[m] = attention_wide(model,rel_model_name[m],layer_name,XT_train,np.argmax(y_train,axis=1),\n",
        "                                      norm_c=False,norm_max_min=False,plot_int=False,transpose=True)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/GradCam_Paper/GigaData/results/resulting_attention_maps/score_attmaps_'+str(n_sbj[sbj])+'.pickle', 'wb') as f:\n",
        "            pickle.dump([relM_, tmpr_], f)\n",
        "  #-----------------------------------------------------------------------------\n",
        "  del model\n",
        "  #-----------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}