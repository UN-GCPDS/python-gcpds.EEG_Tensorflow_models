{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UN-GCPDS/python-gcpds.EEG_Tensorflow_models/blob/main/Examples/BCI2a/shallowconvnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLD2zi6MxgIf"
      },
      "source": [
        "# BCI2a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T17:12:50.456171Z",
          "iopub.status.busy": "2021-09-26T17:12:50.454128Z",
          "iopub.status.idle": "2021-09-26T17:13:06.014392Z",
          "shell.execute_reply": "2021-09-26T17:13:06.013579Z",
          "shell.execute_reply.started": "2021-09-26T17:12:50.456088Z"
        },
        "id": "gb0esQltb-hG",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git\n",
            "  Cloning https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git to /tmp/pip-req-build-s41uqryd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git /tmp/pip-req-build-s41uqryd\n",
            "  Resolved https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git to commit 17345e80529682bf67d8845b0e76a44bfbbcf282\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting braindecode@ git+https://github.com/braindecode/braindecode\n",
            "  Cloning https://github.com/braindecode/braindecode to /tmp/pip-install-nx2o97aq/braindecode_4d8539d353c9402c8400964b0924adb1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/braindecode/braindecode /tmp/pip-install-nx2o97aq/braindecode_4d8539d353c9402c8400964b0924adb1\n",
            "  Resolved https://github.com/braindecode/braindecode to commit 5b4d8daee842c72fbd463b9d4528957fcd32f7be\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting moabb@ git+https://github.com/UN-GCPDS/moabb.git\n",
            "  Cloning https://github.com/UN-GCPDS/moabb.git to /tmp/pip-install-nx2o97aq/moabb_1fcc73c712ec43fb82bd03da1283cb34\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/UN-GCPDS/moabb.git /tmp/pip-install-nx2o97aq/moabb_1fcc73c712ec43fb82bd03da1283cb34\n",
            "  Resolved https://github.com/UN-GCPDS/moabb.git to commit 57e31a78ab21cb27aad4182d76133303ed574ce9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m520.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting mne\n",
            "  Downloading mne-0.24.1-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m352.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Using cached numpy-1.22.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "Collecting h5py\n",
            "  Using cached h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "Collecting skorch\n",
            "  Using cached skorch-0.11.0-py3-none-any.whl (155 kB)\n",
            "Collecting typeguard>=2.7\n",
            "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/daniel/anaconda3/envs/gcpds/lib/python3.8/site-packages (from matplotlib->braindecode@ git+https://github.com/braindecode/braindecode->EEG-Tensorflow-models==0.1) (2.8.2)\n",
            "Collecting pyparsing>=2.2.1\n",
            "  Using cached pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Using cached fonttools-4.29.1-py3-none-any.whl (895 kB)\n",
            "Collecting pillow>=6.2.0\n",
            "  Using cached Pillow-9.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "Collecting scikit-learn>=0.19.1\n",
            "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m446.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.14.0\n",
            "  Using cached tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
            "Collecting tabulate>=0.7.7\n",
            "  Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/daniel/anaconda3/envs/gcpds/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->braindecode@ git+https://github.com/braindecode/braindecode->EEG-Tensorflow-models==0.1) (1.16.0)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: EEG-Tensorflow-models, braindecode, moabb\n",
            "  Building wheel for EEG-Tensorflow-models (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for EEG-Tensorflow-models: filename=EEG_Tensorflow_models-0.1-py3-none-any.whl size=3993 sha256=0956259e7d1eade92369ed823de0214b8401d31bff00c40139b075d18edb4dd0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w0wgvqxq/wheels/44/65/63/e2980a756cecfd5ef9c41fc1ae61256749dd407ca87be95ca6\n",
            "  Building wheel for braindecode (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for braindecode: filename=Braindecode-0.6-py3-none-any.whl size=177385 sha256=94e572b7c4658c1ca87774454974540e04233dcbe533df67364fc47fee6d03e5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w0wgvqxq/wheels/67/90/ad/e475dd5e1b89f92aa3cbffd0fb44df0e91e7c900bc3a234238\n",
            "  Building wheel for moabb (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for moabb: filename=moabb-0.1-py3-none-any.whl size=10217 sha256=79a0d982914d63da2afd50c90d8db35313dfc3285328f5aec77325b42a8d5aec\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w0wgvqxq/wheels/2c/ad/e1/06b70334ed96cc011fd2c60f69e54beb089444a0afc5a3b7dd\n",
            "Successfully built EEG-Tensorflow-models braindecode moabb\n",
            "Installing collected packages: tabulate, pytz, typeguard, tqdm, threadpoolctl, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, tensorflow-addons, scipy, pandas, packaging, moabb, h5py, scikit-learn, mne, matplotlib, skorch, braindecode, EEG-Tensorflow-models\n",
            "Successfully installed EEG-Tensorflow-models-0.1 braindecode-0.6 cycler-0.11.0 fonttools-4.29.1 h5py-3.6.0 joblib-1.1.0 kiwisolver-1.3.2 matplotlib-3.5.1 mne-0.24.1 moabb-0.1 numpy-1.22.2 packaging-21.3 pandas-1.4.1 pillow-9.0.1 pyparsing-3.0.7 pytz-2021.3 scikit-learn-1.0.2 scipy-1.8.0 skorch-0.11.0 tabulate-0.8.9 tensorflow-addons-0.16.1 threadpoolctl-3.1.0 tqdm-4.63.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "%pip install -U git+https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T17:13:06.016428Z",
          "iopub.status.busy": "2021-09-26T17:13:06.016145Z",
          "iopub.status.idle": "2021-09-26T17:13:16.770073Z",
          "shell.execute_reply": "2021-09-26T17:13:16.769348Z",
          "shell.execute_reply.started": "2021-09-26T17:13:06.016399Z"
        },
        "id": "VFg-1frxcagM",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'EEG_Tensorflow_models.Utils'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/daniel/Documents/python-gcpds.EEG_Tensorflow_models/Examples/BCI2a/shallowconvnet.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daniel/Documents/python-gcpds.EEG_Tensorflow_models/Examples/BCI2a/shallowconvnet.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mEEG_Tensorflow_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mUtils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mLoadData\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniel/Documents/python-gcpds.EEG_Tensorflow_models/Examples/BCI2a/shallowconvnet.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mEEG_Tensorflow_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mUtils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mCallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m get_callbacks\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniel/Documents/python-gcpds.EEG_Tensorflow_models/Examples/BCI2a/shallowconvnet.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mEEG_Tensorflow_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mUtils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mTrainingModels\u001b[39;00m \u001b[39mimport\u001b[39;00m get_loss,get_model,get_optimizer,train_model_cv\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'EEG_Tensorflow_models.Utils'"
          ]
        }
      ],
      "source": [
        "from EEG_Tensorflow_models.Utils.LoadData import load_dataset\n",
        "from EEG_Tensorflow_models.Utils.Callbacks import get_callbacks\n",
        "from EEG_Tensorflow_models.Utils.TrainingModels import get_loss,get_model,get_optimizer,train_model_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T17:13:16.77197Z",
          "iopub.status.busy": "2021-09-26T17:13:16.771145Z",
          "iopub.status.idle": "2021-09-26T17:13:16.830566Z",
          "shell.execute_reply": "2021-09-26T17:13:16.829975Z",
          "shell.execute_reply.started": "2021-09-26T17:13:16.77194Z"
        },
        "id": "HGbDskCzfluR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.signal import resample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yER2E281fKti"
      },
      "source": [
        "# Exp 1: Schirmeister 2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T17:27:36.055302Z",
          "iopub.status.busy": "2021-09-26T17:27:36.054952Z",
          "iopub.status.idle": "2021-09-26T17:30:03.568757Z",
          "shell.execute_reply": "2021-09-26T17:30:03.56567Z",
          "shell.execute_reply.started": "2021-09-26T17:27:36.055268Z"
        },
        "id": "7k3LXi8vcVgI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "opt_args = {'lr': 0.01,'beta_1': 0.9}\n",
        "optimizer = get_optimizer('Adam',opt_args)\n",
        "\n",
        "loss = get_loss('CategoricalCrossentropy')\n",
        "\n",
        "metrics = ['accuracy']\n",
        "\n",
        "callbacks_names = {'early_stopping_train':'early_stopping','checkpoint_train':'checkpoint',\n",
        "                'Threshold_valid':'Threshold','checkpoint_valid':'checkpoint',\n",
        "                'early_stopping_valid':'early_stopping'}\n",
        "\n",
        "\n",
        "Experiment = 'schirrmeister2017'\n",
        "model_name = 'ShallowConvNet'\n",
        "Version='2018'\n",
        "\n",
        "subjects = np.arange(1,10)\n",
        "Acc = []\n",
        "History = []\n",
        "Subject = []\n",
        "fs_new = 128\n",
        "for sbj in subjects:\n",
        "    print('Subject: {:d} of {:d}'.format(sbj,len(subjects)))\n",
        "\n",
        "    X_train,y_train,X_valid,y_valid,fs = load_dataset(dataset_name=\"BNCI2014001\", subject_id=sbj)\n",
        "    \n",
        "    X_train = resample(X_train,int((X_train.shape[-2]/fs)*fs_new),axis=-2)    \n",
        "    X_valid = resample(X_valid,int((X_valid.shape[-2]/fs)*fs_new),axis=-2)     \n",
        "    \n",
        "\n",
        "    model_args = {'nb_classes':4,'Chans':X_train.shape[1],'Samples':X_train.shape[2],'dropoutRate':0.5,'version':Version}\n",
        "    model = get_model(model_name,model_args)\n",
        "    \n",
        "   \n",
        "\n",
        "    #model_args = {'nb_classes':4,'Chans':X_train.shape[1],'Samples':X_train.shape[2],'dropoutRate':0.5,'version':'2017'}\n",
        "    #model = get_model(model_name,model_args)\n",
        "    \n",
        "    call_args = [{'monitor':'val_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "                {'filepath':'/kaggle/tmp/checkpoint_sbj_'+str(sbj),'save_format':'tf','monitor':'val_accuracy','verbose':1,'save_weights_only':True,'save_best_only':True},\n",
        "                {'threshold':None},\n",
        "                {'filepath':'/kaggle/tmp/checkpoint_2_sbj_'+str(sbj),'save_format':'tf','monitor':'val_accuracy','verbose':1,'save_weights_only':True,'save_best_only':True},\n",
        "                {'monitor':'val_accuracy','patience':None,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':True}]\n",
        "                \n",
        "    callbacks = get_callbacks(callbacks_names,call_args)\n",
        "\n",
        "    cv = train_model_cv(model,optimizer,loss,metrics,callbacks=callbacks)\n",
        "\n",
        "    history = cv.fit_validation(X_train,y_train,X_val=X_valid,y_val=y_valid,batch_size=64,epochs=1000,verbose=1,val_mode=Experiment)\n",
        "    acc = cv.get_accuracy()\n",
        "    print('Subject accuracy: {:f}'.format(acc))\n",
        "    Acc.append(acc)\n",
        "    History.append(History)\n",
        "    Subject.append(sbj)\n",
        "\n",
        "    results = {}\n",
        "    results['subject'] = Subject\n",
        "    results['history'] = History\n",
        "    results['accuracy'] = Acc\n",
        "\n",
        "    with open('Results_BCI2a_'+Experiment+'_'+model_name+Version+'.p','wb') as handle:\n",
        "        pickle.dump(results,handle)\n",
        "    \n",
        "    del cv,callbacks,X_train,y_train,X_valid,y_valid,fs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkmyULdHpERD"
      },
      "source": [
        "#Exp 2: Schirmeister 2017_legal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-24T03:18:13.110208Z",
          "iopub.status.idle": "2021-09-24T03:18:13.110692Z",
          "shell.execute_reply": "2021-09-24T03:18:13.11046Z",
          "shell.execute_reply.started": "2021-09-24T03:18:13.110427Z"
        },
        "id": "KxxJHDIJfjhr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "opt_args = {'lr': 0.01,'beta_1': 0.9}\n",
        "optimizer = get_optimizer('Adam',opt_args)\n",
        "\n",
        "loss = get_loss('CategoricalCrossentropy')\n",
        "\n",
        "metrics = ['accuracy']\n",
        "\n",
        "callbacks_names = {'early_stopping_train':'early_stopping','checkpoint_train':'checkpoint',\n",
        "                'Threshold_valid':'Threshold','checkpoint_valid':'checkpoint',\n",
        "                'early_stopping_valid':'early_stopping'}\n",
        "\n",
        "\n",
        "Experiment = 'schirrmeister2017_legal'\n",
        "model_name = 'ShallowConvNet'\n",
        "Version='2018'\n",
        "\n",
        "subjects = np.arange(1,10)\n",
        "Acc = []\n",
        "History = []\n",
        "Subject = []\n",
        "for sbj in subjects:\n",
        "    print('Subject: {:d} of {:d}'.format(sbj,len(subjects)))\n",
        "\n",
        "    X_train,y_train,X_valid,y_valid,fs = load_dataset(dataset_name=\"BNCI2014001\", subject_id=sbj)\n",
        "    fs_new = 128\n",
        "    X_train = resample(X_train,int((X_train.shape[-2]/fs)*fs_new),axis=-2)    \n",
        "    X_valid = resample(X_valid,int((X_valid.shape[-2]/fs)*fs_new),axis=-2)    \n",
        "    model_args = {'nb_classes':4,'Chans':X_train.shape[1],'Samples':X_train.shape[2],'dropoutRate':0.5,'version':Version}\n",
        "    model = get_model(model_name,model_args)\n",
        "\n",
        "\n",
        "    call_args = [{'monitor':'val_accuracy','patience':100,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':False},\n",
        "                {'filepath':'/kaggle/tmp/checkpoint_sbj_'+str(sbj),'save_format':'tf','monitor':'val_accuracy','verbose':1,'save_weights_only':True,'save_best_only':True},\n",
        "                {'threshold':None},\n",
        "                {'filepath':'/kaggle/tmp/checkpoint_2_sbj_'+str(sbj),'save_format':'tf','monitor':'val_accuracy','verbose':1,'save_weights_only':True,'save_best_only':True},\n",
        "                {'monitor':'val_accuracy','patience':None,'min_delta':0.001,'mode':'max','verbose':1,'restore_best_weights':True}]\n",
        "                \n",
        "    callbacks = get_callbacks(callbacks_names,call_args)\n",
        "\n",
        "    cv = train_model_cv(model,optimizer,loss,metrics,callbacks=callbacks)\n",
        "\n",
        "    history = cv.fit_validation(X_train,y_train,X_val=X_valid,y_val=y_valid,batch_size=64,epochs=1000,verbose=1,val_mode=Experiment)\n",
        "    acc = cv.get_accuracy()\n",
        "    print('Subject accuracy: {:f}'.format(acc))\n",
        "    Acc.append(acc)\n",
        "    History.append(History)\n",
        "    Subject.append(sbj)\n",
        "\n",
        "    results = {}\n",
        "    results['subject'] = Subject\n",
        "    results['history'] = History\n",
        "    results['accuracy'] = Acc\n",
        "\n",
        "    with open('Results_BCI2a_'+Experiment+'_'+model_name+Version+'.p','wb') as handle:\n",
        "        pickle.dump(results,handle)\n",
        "    \n",
        "    del cv,callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc57RFoppu6I"
      },
      "source": [
        "# Exp 3: Schirmeister 2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-24T03:18:13.112271Z",
          "iopub.status.idle": "2021-09-24T03:18:13.112757Z",
          "shell.execute_reply": "2021-09-24T03:18:13.112522Z",
          "shell.execute_reply.started": "2021-09-24T03:18:13.112497Z"
        },
        "id": "e3zhvmwHpyJ2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "opt_args = {'lr': 0.01,'beta_1': 0.9}\n",
        "optimizer = get_optimizer('Adam',opt_args)\n",
        "\n",
        "loss = get_loss('CategoricalCrossentropy')\n",
        "\n",
        "metrics = ['accuracy']\n",
        "\n",
        "callbacks_names = {'checkpoint_valid':'checkpoint',\n",
        "                   'early_stopping_valid':'early_stopping'}\n",
        "\n",
        "\n",
        "Experiment = 'schirrmeister2021'\n",
        "model_name = 'ShallowConvNet'\n",
        "Version='2018'\n",
        "\n",
        "subjects = np.arange(1,10)\n",
        "Acc = []\n",
        "History = []\n",
        "Subject = []\n",
        "for sbj in subjects:\n",
        "    print('Subject: {:d} of {:d}'.format(sbj,len(subjects)))\n",
        "\n",
        "    X_train,y_train,X_valid,y_valid,fs = load_dataset(dataset_name=\"BNCI2014001\", subject_id=sbj)\n",
        "    fs_new = 128\n",
        "    X_train = resample(X_train,int((X_train.shape[-2]/fs)*fs_new),axis=-2)    \n",
        "    X_valid = resample(X_valid,int((X_valid.shape[-2]/fs)*fs_new),axis=-2)    \n",
        "    model_args = {'nb_classes':4,'Chans':X_train.shape[1],'Samples':X_train.shape[2],'dropoutRate':0.5,'version':Version}\n",
        "    model = get_model(model_name,model_args)\n",
        "    \n",
        "    \n",
        "\n",
        "    call_args = [\n",
        "            {'filepath':'/kaggle/tmp/checkpoint_'+str(sbj),\n",
        "            'save_format':'tf',\n",
        "            'monitor':'val_accuracy',\n",
        "            'verbose':1,\n",
        "            'save_weights_only':True,\n",
        "            'save_best_only':True},\n",
        "            {'monitor':'val_accuracy',\n",
        "            'patience':100,\n",
        "            'min_delta':0.001,\n",
        "            'mode':'max',\n",
        "            'verbose':1,\n",
        "            'restore_best_weights':True}]\n",
        "                \n",
        "    callbacks = get_callbacks(callbacks_names,call_args)\n",
        "\n",
        "    cv = train_model_cv(model,optimizer,loss,metrics,callbacks=callbacks)\n",
        "\n",
        "    history = cv.fit_validation(X_train,y_train,X_val=X_valid,y_val=y_valid,batch_size=64,epochs=1000,verbose=1,val_mode=Experiment)\n",
        "    acc = cv.get_accuracy()\n",
        "    print('Subject accuracy: {:f}'.format(acc))\n",
        "    Acc.append(acc)\n",
        "    History.append(History)\n",
        "    Subject.append(sbj)\n",
        "\n",
        "    results = {}\n",
        "    results['subject'] = Subject\n",
        "    results['history'] = History\n",
        "    results['accuracy'] = Acc\n",
        "\n",
        "    with open('Results_BCI2a_'+Experiment+'_'+model_name+Version+'.p','wb') as handle:\n",
        "        pickle.dump(results,handle)\n",
        "    \n",
        "    del cv,callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AYFMYo_qWks"
      },
      "source": [
        "# Exp 4: 4-fold CV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-24T03:18:13.114398Z",
          "iopub.status.idle": "2021-09-24T03:18:13.114881Z",
          "shell.execute_reply": "2021-09-24T03:18:13.114657Z",
          "shell.execute_reply.started": "2021-09-24T03:18:13.114632Z"
        },
        "id": "FPWVi22jqf-U",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "opt_args = {'lr': 0.01,'beta_1': 0.9}\n",
        "optimizer = get_optimizer('Adam',opt_args)\n",
        "\n",
        "loss = get_loss('CategoricalCrossentropy')\n",
        "\n",
        "metrics = ['accuracy']\n",
        "\n",
        "callbacks_names = {'checkpoint_train1':'checkpoint','checkpoint_train2':'checkpoint','checkpoint_train3':'checkpoint','checkpoint_train4':'checkpoint'}\n",
        "\n",
        "\n",
        "Experiment = 'lawhern2018'\n",
        "model_name = 'ShallowConvNet'\n",
        "Version='2018'\n",
        "\n",
        "fs_new = 128\n",
        "\n",
        "\n",
        "subjects = np.arange(1,10)\n",
        "Acc = []\n",
        "History = []\n",
        "Subject = []\n",
        "for sbj in subjects:\n",
        "    print('Subject: {:d} of {:d}'.format(sbj,len(subjects)))\n",
        "\n",
        "    X_train,y_train,X_valid,y_valid,fs = load_dataset(dataset_name=\"BNCI2014001\", subject_id=sbj)\n",
        "    fs_new = 128\n",
        "    X_train = resample(X_train,int((X_train.shape[-2]/fs)*fs_new),axis=-2)    \n",
        "    X_valid = resample(X_valid,int((X_valid.shape[-2]/fs)*fs_new),axis=-2)    \n",
        "    model_args = {'nb_classes':4,'Chans':X_train.shape[1],'Samples':X_train.shape[2],'dropoutRate':0.5,'version':Version}\n",
        "    model = get_model(model_name,model_args)\n",
        "\n",
        "    call_args = [\n",
        "            \n",
        "            {'filepath':'/kaggle/tmp/checkpoint1_'+str(sbj),\n",
        "            'save_format':'tf',\n",
        "            'monitor':'val_accuracy',\n",
        "            'verbose':1,\n",
        "            'save_weights_only':True,\n",
        "            'save_best_only':True},\n",
        "             {'filepath':'/kaggle/tmp/checkpoint2_'+str(sbj),\n",
        "            'save_format':'tf',\n",
        "            'monitor':'val_accuracy',\n",
        "            'verbose':1,\n",
        "            'save_weights_only':True,\n",
        "            'save_best_only':True},\n",
        "             {'filepath':'/kaggle/tmp/checkpoint3_'+str(sbj),\n",
        "            'save_format':'tf',\n",
        "            'monitor':'val_accuracy',\n",
        "            'verbose':1,\n",
        "            'save_weights_only':True,\n",
        "            'save_best_only':True},\n",
        "             {'filepath':'/kaggle/tmp/checkpoint4_'+str(sbj),\n",
        "            'save_format':'tf',\n",
        "            'monitor':'val_accuracy',\n",
        "            'verbose':1,\n",
        "            'save_weights_only':True,\n",
        "            'save_best_only':True}]\n",
        "                \n",
        "    callbacks = get_callbacks(callbacks_names,call_args)\n",
        "\n",
        "    cv = train_model_cv(model,optimizer,loss,metrics,callbacks=callbacks)\n",
        "\n",
        "    history = cv.fit_validation(X_train,y_train,X_val=X_valid,y_val=y_valid,batch_size=64,epochs=1000,verbose=1,val_mode=Experiment)\n",
        "    acc = cv.get_accuracy()\n",
        "    print('Subject accuracy: {:f}'.format(acc))\n",
        "    Acc.append(acc)\n",
        "    History.append(History)\n",
        "    Subject.append(sbj)\n",
        "\n",
        "    results = {}\n",
        "    results['subject'] = Subject\n",
        "    results['history'] = History\n",
        "    results['accuracy'] = Acc\n",
        "\n",
        "    with open('Results_BCI2a_'+Experiment+'_'+model_name+Version+'.p','wb') as handle:\n",
        "        pickle.dump(results,handle)\n",
        "    \n",
        "    del cv,callbacks"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "shallowconvnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
